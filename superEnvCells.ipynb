{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc828ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some self variables that you could use in the real environment class\n",
    "class SuperEnv(gym.Env):\n",
    "    \n",
    "    # Define your action and observation spaces. You want to take actions in one dimension, changing the h42 phase offset,\n",
    "    # and want a 4D observation, consisting of your relative bunch lengths and bunch intensities. Both your observations and \n",
    "    # actions will be should be normalized to be within [-1,1] for optimisation reasons.\n",
    "    \n",
    "    metadata= {'render.modes': ['human']}\n",
    "    \n",
    "    action_space = gym.spaces.Box(\n",
    "                                low = np.array([-1,]),#__, # Fill in the __ with your action settings.\n",
    "                                high = np.array([1,]),#__,\n",
    "                                shape=(1,),\n",
    "                                dtype=np.float32)\n",
    "\n",
    "\n",
    "    ### Define what the observations are to be expected\n",
    "    observation_space = gym.spaces.Box(\n",
    "                                low = np.array([-1,-1,-1,-1]), # Fill in the __ with your observation settings.\n",
    "                                high = np.array([1,1,1,1]),\n",
    "                                shape=(4,),\n",
    "                                dtype=np.float32)\n",
    "    \n",
    "    ### Stop criteria (constituting \"good\" splittings with provided difference estimate).\n",
    "    BUNCH_LENGTH_INT_CRITERIA = 0.02 # Empirically evaluated diff_estimate that constitutes a \"good\" bunch splitting. Lower means longer training time, but smaller spread in bunch lengths/intensities.\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def __init__(self,\n",
    "                max_steps = 100,\n",
    "                max_step_size = 20,\n",
    "                min_setting = -45,\n",
    "                max_setting = 45,):\n",
    "        \n",
    "        ### Assign hyperparameter settings to attributes\n",
    "        self.max_steps = max_steps\n",
    "        self.max_step_size = max_step_size\n",
    "        self.min_setting = min_setting\n",
    "        self.max_setting = max_setting\n",
    "        \n",
    "        ### Status of the iterations\n",
    "        # Steps, initializing lists to store actions/states/rewards...\n",
    "        \n",
    "        self.counter = 0 # Counts the number of steps taken in the environment!\n",
    "        self.curr_step = -1  ## Not used ?\n",
    "        self.phase_correction = 0\n",
    "\n",
    "        # Initialize lists for tracking episodes\n",
    "        self.curr_episode = -1\n",
    "        self.action_episode_memory = []\n",
    "        self.diff_estimate_memory = []\n",
    "        self.state_memory = []\n",
    "        self.phase_set_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.is_finalized = False\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        One step/action in the environment, returning the observable\n",
    "        and reward. \n",
    "\n",
    "        Stopping conditions: max_steps reached, or splitting good enough.\n",
    "        \"\"\"\n",
    "        success = False    \n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code below!!\n",
    "        ################################################################################################################################################################\n",
    "        \n",
    "        # Hint: Number of steps taken in environment is tracked in self.counter\n",
    "        \n",
    "        \n",
    "        self._take_action(action) # Actually take action: Define your _take_action function below!! \n",
    "        reward = self._get_reward() # Get your reward: Define your _get_reward function below!! Returns a reward value.\n",
    "        \n",
    "        state = self.state\n",
    "        ### Check exit criteria: Achieved good enough state, or taken too many steps.\n",
    "        ### BUNCH_LENGHT_INT_CRITERIA based on the given diff_estimate value.\n",
    "        \n",
    "        if abs(self.diff_estimate) < self.BUNCH_LENGTH_INT_CRITERIA: # Check if the diff_estimate is below criterion. If so, episode finalized and a success!\n",
    "            self.is_finalized = True \n",
    "            success = True\n",
    "        # print(self.counter)    \n",
    "        if self.counter >= self.max_steps: # Check if you have exceeded the maximum step limit. If so, episode finalized but not a success...\n",
    "            self.is_finalized = True\n",
    "\n",
    "        # Here you can add any extra info you would like to be returned on each step, e.g. episode steps, rewards, actions etc.\n",
    "        info = {'success': success, 'steps': self.counter, 'profile': self.profile} \n",
    "        \n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code above!!\n",
    "        ################################################################################################################################################################\n",
    "        \n",
    "        \n",
    "        return state, reward, self.is_finalized, info # Standardized output according to gym framework.\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        \"\"\"\n",
    "        Actual action funtion.\n",
    "\n",
    "        Action from model is scaled to be between [-1,1] for better optimization performance. \n",
    "        Converted back to phase setting in degrees using self.max_step_size.\n",
    "        \n",
    "        Args:\n",
    "            action (ndarray): n-dimensional action. Datatype, dimension, and value ranges defined in self.action_space.\n",
    "        \"\"\"\n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code below!!\n",
    "        ################################################################################################################################################################\n",
    "\n",
    "        converted_action = action*self.max_step_size # Convert your action from your normalized range [-1,1] back into degrees using self.max_step_size. HINT: action*self.max_step_size = what?\n",
    "        self.phase_correction += converted_action # Phase correction tracks previous actions taken to get the cumulative change from start.\n",
    "\n",
    "        # Phase offset as action, add offset to current phase_set to get next setting. This is what defines which simulated datapoint to collect in your _get_state function!!!\n",
    "        self.phase_set += converted_action\n",
    "\n",
    "        # Update the self.state parameter with the new state. The preprovided self._get_state() will provide you with a state based on the current self.phase_set attribute value.\n",
    "        # The state will consist of a vector of 4 values: [bunch_length_1, bunch_length_2, bunch_intensity_1, bunch_intensity_2].\n",
    "        self.state = self._get_state()\n",
    "\n",
    "\n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code above!!\n",
    "        ################################################################################################################################################################  \n",
    "        curr_state = self.state.copy()\n",
    "        curr_phase_set = self.phase_set.copy()\n",
    "        self.action_episode_memory.append(action)\n",
    "        self.state_memory.append(curr_state)\n",
    "        self.phase_set_memory.append(curr_phase_set)\n",
    "        \n",
    "        self.counter += 1\n",
    "\n",
    "     \n",
    "    \n",
    "    def _get_state(self):\n",
    "        '''\n",
    "        Get the observable for a given phase_set. This function is provided completed to help you collect datapoints from the pre-simulated dataset.\n",
    "        The data_class class is written to provide datapoints from a quadsplit dataset, but by always providing h84=0 we only vary the first phase\n",
    "        offset.\n",
    "\n",
    "        Comment: The edge cases of trying to move to datapoints outside the simulated dataset needs to be handled.\n",
    "        Currently it is simply checked whether the phase setting is above the max setting or below the min setting,\n",
    "        and if so a pre-defined dummy observation is presented. The important factor to consider is to make sure that\n",
    "        all edge cases are covered by some dummy state, and that the dummy states are unique (so the model can learn\n",
    "        what steps to take to get back in the right search area). It is also highly advised to give an additional penalty\n",
    "        in the reward if the agent steps outside our region of simulated data.\n",
    "        '''\n",
    "        \n",
    "        ### Check whether we are within simulated settings\n",
    "        if (self.phase_set<self.min_setting):\n",
    "            state = np.array([0.5, -0.5, 0.5, -0.5])\n",
    "        elif (self.phase_set>self.max_setting):\n",
    "            state = np.array([-0.5, 0.5, -0.5, 0.5])\n",
    "        else:\n",
    "            \n",
    "            ################################################################################################################################################################\n",
    "            # Implement the code below!!\n",
    "            #########################################################################################################################\n",
    "            \n",
    "            # Collecting the simulated datapoint, calculating state description\n",
    "            profile = data_class.get_interpolated_profile(self.phase_set) # Second phase does not affect the first. Since we only care about h42, no need to assign h84 offfset.\n",
    "            state = get_state_from_profile(profile)\n",
    "            \n",
    "            ################################################################################################################################################################\n",
    "            # Implement the code above!!\n",
    "            ################################################################################################################################################################\n",
    "        \n",
    "            self.profile = profile # Add a tracking of the profile for plotting purposes!\n",
    "        return state.astype(np.float32)\n",
    "    \n",
    "    def _get_reward(self):\n",
    "        \"\"\" Evaluating the reward from the observable/state. \n",
    "            The example reward 'simple_profile' is provided, and is based on the final profile\n",
    "            after the splitting. This is using more information than just the state provided\n",
    "            to the agent.\n",
    "\n",
    "            Feel free to experiment and design your own reward as well! \n",
    "\n",
    "        Returns:\n",
    "            float: The reward based on the current state. \n",
    "        \"\"\" \n",
    "        \n",
    "        \n",
    "        \n",
    "        # The shape of your observable should match your optimization_space. For this excercise, it is expected\n",
    "        # that you use an observation space of a vector with four values\n",
    "        observable = self.state\n",
    "\n",
    "        diff_estimate = get_diff_estimate_from_state(observable)\n",
    "        self.diff_estimate = diff_estimate\n",
    "\n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code below!!\n",
    "        ################################################################################################################################################################\n",
    "        \"\"\" \n",
    "         Define your own reward here. The diff_estimate provided above is provided \n",
    "         as a simple way to define the difference between your bunches length/intensity \n",
    "         after the splitting. You want this to be as small as possible, so a better reward\n",
    "         should be given for a smaller diff_estimate. There is already a pre-defined\n",
    "         criterion for the diff_estimate to be considered a \"good\" splitting provided in\n",
    "         the BUNCH_LENGTH_INT_CRITERIA attribute. \n",
    "         \"\"\"\n",
    "        \n",
    "        #step-wise sparse reward: Reward reaching good conditions fast, steps that do not significantly\n",
    "        # improve our state give no additional reward.\n",
    "        \n",
    "        reward = -1 # Every step is bad: negative reward is the baseline.\n",
    "\n",
    "        if (self.phase_set<self.min_setting) or (self.phase_set>self.max_setting):\n",
    "                reward += -4 # If outside our region of interest, give large penalty.\n",
    "            \n",
    "        elif abs(diff_estimate) < self.BUNCH_LENGTH_INT_CRITERIA: # Tested to see where it is close to optimal setting\n",
    "            reward += 101 # If we reach our goal, the agent is heavily incentivized.\n",
    "        elif abs(diff_estimate) < 0.025:\n",
    "            reward += 0.75\n",
    "        elif abs(diff_estimate) < 0.2:\n",
    "            reward += 0.5\n",
    "        elif abs(diff_estimate) < 0.5:\n",
    "            reward += 0.25\n",
    "        #reward = -diff_estimate #______ # Define your own reward here!! \n",
    "\n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code above!!\n",
    "        ################################################################################################################################################################\n",
    "        \n",
    "        ### Tracking of diff_estimate. Lets you use my render() function to observe your agent.\n",
    "        curr_diff_estimate = self.diff_estimate.copy()\n",
    "        self.diff_estimate_memory.append(curr_diff_estimate)\n",
    "        self.reward_memory.append(reward)\n",
    "            \n",
    "        return reward\n",
    "       \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset to a random state to start over a training episode.\n",
    "        This function will be called everytime an episode is started\n",
    "        to provide an initial state.\n",
    "        \n",
    "        Returns:\n",
    "            state: The inital state of the environment. Should match the shape defined in observation_space.\n",
    "        \"\"\"\n",
    "        # Resetting to start a new episode\n",
    "        self.curr_episode += 1\n",
    "        self.counter = 0\n",
    "        self.is_finalized = False # This tracks wether an episode is complete or not.\n",
    "        \n",
    "        #print(f'Resetting for episode {self.curr_episode}')\n",
    "\n",
    "        # Initializing episode lists to track data for individual episodes. Some used for rendering.\n",
    "        self.action_episode_memory = []\n",
    "        self.state_memory= []\n",
    "        self.phase_set_memory= []\n",
    "        self.reward_memory= []\n",
    "        self.diff_estimate_memory= []\n",
    "        \n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code below!!\n",
    "        ################################################################################################################################################################\n",
    "        \n",
    "        # Getting initial state\n",
    "        \n",
    "        # Initialize random starting phase_set within [self.min_setting, self.max_setting], for example using random.uniform(min,max)\n",
    "        self.phase_set = random.uniform(self.min_setting,\n",
    "                                            self.max_setting)\n",
    "        \n",
    "        self.initial_offset = np.copy(self.phase_set)\n",
    "        self.phase_correction = 0\n",
    "                                        \n",
    "        self.state = self._get_state() # call _get_state to get the initial state from the starting phase_set.\n",
    "        state = self.state\n",
    "        \n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code above!!\n",
    "        ################################################################################################################################################################\n",
    " \n",
    "        ### Some tracking of state, phase, reward, diff_estimate. Lets you use my render() function to observe your agent.\n",
    "        self.state_memory.append(state)\n",
    "        self.phase_set_memory.append(self.phase_set)\n",
    "        reward = self._get_reward()\n",
    "        self.reward_memory.append(reward)\n",
    "        \n",
    "        return state.astype(np.float32)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"\n",
    "        Set the random seed. Useful if you want to standardize trainings.\n",
    "        \"\"\"\n",
    "        \n",
    "        random.seed(seed)\n",
    "        np.random.seed\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Rendering function meant to provide a human-readable output. Base function in gym\n",
    "        environments to override. I provide a simple version that should let you observe \n",
    "        your trained agent during evaluation.\n",
    "        \"\"\"\n",
    "        plt.figure('Agent')\n",
    "        plt.clf()\n",
    "        plt.subplot(131)\n",
    "        plt.suptitle(f'Episode {self.curr_episode}')\n",
    "        plt.title('Current profile')\n",
    "        plt.plot(self.profile,'b')\n",
    "        plt.subplot(132)\n",
    "        plt.title('Difference estimate')\n",
    "        plt.plot(self.diff_estimate_memory, 'o-')\n",
    "        plt.axhline(y=self.BUNCH_LENGTH_INT_CRITERIA, color='k', linestyle='--')\n",
    "        plt.subplot(133)\n",
    "        plt.title('h42 phase offset')\n",
    "        plt.plot(np.asarray(self.phase_set_memory, dtype=object), 'go-')\n",
    "        plt.axhline(y=0, color='k', linestyle='--')\n",
    "        plt.ylim((-30,30))\n",
    "        \n",
    "        #plot_finish(fig=fig, axes=axes, xlabel='Setting', ylabel='Observable')\n",
    "        plt.pause(0.2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d2177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model\n",
    "\n",
    "# Separate evaluation env\n",
    "eval_env = SuperEnv(max_step_size=45)\n",
    "\n",
    "# Use deterministic actions for evaluation\n",
    "eval_callback = EvalCallback(eval_env,  best_model_save_path='./RL_logs/SAC/Super_lims_45',\n",
    "    log_path='./RL_logs/', eval_freq=1000,\n",
    "    deterministic=True, render=False)\n",
    "\n",
    "\n",
    "env = SuperEnv(max_step_size=45)\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "log_dir = './'\n",
    "\n",
    "\n",
    "ent_coef = 1/5#'auto' # Hyperparameter of SAC, with 'auto' it is also learned during training.\n",
    "\n",
    "# Define your model\n",
    "sac_model_super = SAC(\"MlpPolicy\", env, verbose=1, ent_coef=ent_coef, learning_starts=100, tensorboard_log=\"./hands-on_rl_tensorboard\")\n",
    "print(\"Starting training...\")\n",
    "sac_model_super.learn(total_timesteps=100000, callback=eval_callback, log_interval=1, tb_log_name=\"hands_on_rl_SAC_super_45\")\n",
    "sac_model_super.save('./saved_models/RL_agent_super_SAC')\n",
    "print(\"Completed training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_environment = SuperEnv()\n",
    "\n",
    "# Set the environment to different states in our observation space, and check the value of the reward.\n",
    "# We will also plot our difference estimate, to see that our reward are reasonable in relation to our\n",
    "# bunch characteristics.\n",
    "\n",
    "phases = np.linspace(-45,45,361)\n",
    "rewards = []\n",
    "diff_estimates = []\n",
    "\n",
    "for phase in phases:\n",
    "    test_environment.reset() # Initialize \n",
    "    test_environment.phase_set = phase # manually change phase setting\n",
    "    test_environment.state = test_environment._get_state() # Manually update the state based on current self.phase_set\n",
    "    reward = test_environment._get_reward() # Manually calculate reward based on current self.state\n",
    "    \n",
    "    rewards.append(reward)\n",
    "    diff_estimates.append(test_environment.diff_estimate) # Also track difference estimates for visited states.\n",
    "\n",
    "# Plotting\n",
    "\n",
    "ax1= plt.subplot(211)\n",
    "plt.title('Reward/diff_estimate given at different phase offsets')\n",
    "plt.plot(rewards, 'r')\n",
    "plt.ylabel('Received reward')\n",
    "ax2 = plt.subplot(212, sharex=ax1)\n",
    "plt.plot(diff_estimates)\n",
    "plt.ylabel('Difference estimate')\n",
    "plt.xticks(np.linspace(0,361,5), np.linspace(-45,45,5))\n",
    "plt.xlabel('Phase offset [deg]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
