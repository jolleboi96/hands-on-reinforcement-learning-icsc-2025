{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd5661c",
   "metadata": {},
   "source": [
    "# For running in SWAN: Loading venv\n",
    "\n",
    "This notebook is possible to run in SWAN, however there are some specific packages needed which are provided in an external virtual environment. However, training agents in SWAN was very slow in my experiments, so I recommend that you follow the instructions in the Gitlab repository to create and install a virtual environment on your local machine, and instead run jupyter notebook from there.\n",
    "\n",
    "If you are running this notebook on your own setup (and not on SWAN), skip the next cell and of straight to importing packages!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15b5c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ONLY IF YOU RUN IN SWAN!!! Not recommended, as training agents will be slow.\n",
    "\n",
    "import os\n",
    "import site\n",
    "import sys\n",
    "venv_location = '/eos/project/s/sy-rf-br/Training/hands-on-reinforcement-learning-2022-venvs/hands-on-rl' #'./hansOn-venv' #/eos/project/s/sy-rf-br/Training/hansOn-venv'\n",
    "venv_site_packages = os.path.join(\n",
    "         venv_location, 'lib', 'python{}.{}'.format(*sys.version_info), 'site-packages')\n",
    "    \n",
    "# If the venv site-packages doesn't exist on the path already, insert it right\n",
    "# before the underlying Python's site-packages.\n",
    "if venv_site_packages not in sys.path:\n",
    "    posn = -1\n",
    "    site_package_dirs = [\n",
    "        index for index, path in enumerate(sys.path) if path.endswith('site-packages')]\n",
    "    print\n",
    "    if site_package_dirs:\n",
    "        posn = site_package_dirs[0]\n",
    "        \n",
    "    sys.path.insert(posn, venv_site_packages)\n",
    "#     print(venv_site_packages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38911ee7",
   "metadata": {},
   "source": [
    "## Import packages and functions - start here if not on SWAN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5543aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from utils import transform_profile, isolate_bunches_from_dm_profile\n",
    "from datamatrix_lookup_class_double import Datamatrix_lookup_class_double\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import gym\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "\n",
    "\n",
    "### Initialize the data provider class to use for illustrative plots in the introduction\n",
    "### This class can provide you with simulated tomoscope acquisitions for phase offsets in \n",
    "### the range [-30,30] degrees in the h=42 harmonic.\n",
    "\n",
    "data_class = Datamatrix_lookup_class_double()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea58ab1",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook we will attempt to train an agent to optimize an RF double splitting (the first part of the quadruple splitting in nominal LHC-type beams in the PS). The optimization problem is one-dimensional, with only one free variable in the phase offset of the $h=42$ cavity. Depending on the phase setting, the splitting will be biased towards one direction and one bunch will contain more particles than the other. The end goal of the optimization is to minimize the differences between the final bunches.\n",
    "\n",
    "## The double splitting: some background\n",
    "\n",
    "As an example problem we will be using an RF-manipulation that is done in the PS during creation of LHC-type beams: a double-splitting. In short, it constitutes taking one bunch placed in a single RF-bucket, and splitting it into two perfectly equal new bunches in their own respective buckets. By equal, we mean that they have the same characteristics; same bunch length, intensity, emmitance etc. In practice this splitting is done by lowering the voltage of the initial RF-cavity (in this case the cavity with harmonic h=21) while raising the voltage of the next one (of double harmonic number, h=42). In so doing we double the amounts of buckets that are available in the accelerator and if done correctly, split the initial bunch into two. \n",
    "\n",
    "<img src=\"./images/double_splitting_info.PNG\" alt=\"profile_input\" width=\"800\"/>\n",
    "\n",
    "However, if the timing between the two RF-cavities is not properly aligned the two end bunches will look very different; you end up with more particles and different characteristics in one bucket compared to the other. This misalignment can be considered a phase offset between the phases of the two RF cavities, and when optimized the phase offset should be zero. This offset will be the parameter that we train our RL agents to minimize, by iteratively taking actions given certain characteristics of the current splitting.\n",
    "\n",
    "## The dataset\n",
    "\n",
    "In order to facilitate the optimization we have our simulated dataset of observations. It is from these observations we need to define some form of metric to determine whether a splitting is good or bad, which could then be used either to guide an optimization algorithm or, as in this notebook, to define a reward to our agents.\n",
    "\n",
    "Due to the time-consuming nature of performing tracking simulations over 10s of thousands turns and the large amounts of training steps generally required in RL, it is not feasible to simulate each function evaluation during training. Because of this a pre-simulated dataset is provided, from which we will sample our datapoints. This does however introduce a few additional challenges that must be considered, e.g.:\n",
    "\n",
    "- **Handling of points outside the dataset** - handled in the environment\n",
    "- **Interpolation between simulated datapoints** - handled by the data class\n",
    "\n",
    "The dataset was created by simulating 361 different phase offsets linearly spaced between -45 and 45 degrees and saving the final bunch lengths, the bunch profile, and the full datamatrix of profiles. We will be using primarily the final bunch profiles and their extracted characteristics (such as relative bunch length/intensity) to train our agents, while the datamatrices will be used for some nice visualizations.\n",
    "\n",
    "### Using the data_class and some example profiles\n",
    "\n",
    "The data_class most important functions are:\n",
    "- <code>get_interpolated_profile(phase)</code> - Returns the final profile after a splitting with the phase offset <code>phase</code>\n",
    "- <code>get_interpolated_matrix(phase)</code> - Returns the entire bunch evolution over the course of the splitting with the phase offset <code>phase</code>\n",
    "\n",
    "Run the code below to see some plots of bunch evolutions and profiles for different phase offsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f213ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of using the dataclass and observing the dataset.\n",
    "# Showing the datamatrix, profiles w/ noise, that we can access\n",
    "# continuous phase offsets such as 5.41...\n",
    "\n",
    "example_phase_offsets = [-45, -30, -15,-5.82, 0, 5.41, 15, 30, 45] # some example offsets, and some phases requiring interpolation.\n",
    "\n",
    "for phase in example_phase_offsets:\n",
    "    # Load interpolated datamatrix or profile\n",
    "    datamatrix = data_class.get_interpolated_matrix(phase)\n",
    "    profile = data_class.get_interpolated_profile(phase)\n",
    "    noisy_profile = transform_profile(profile)\n",
    "    plt.figure()\n",
    "    plt.suptitle(f'Phase offset: {phase} degrees')\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(datamatrix,aspect='auto')\n",
    "    plt.subplot(122)\n",
    "    plt.plot(profile)\n",
    "    plt.subplot(133)\n",
    "    plt.plot(noisy_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8ae6b",
   "metadata": {},
   "source": [
    "The profiles generated by our simulations are much less noisy than real data from the machine. To make our data more similar to a real datapoint, we also have a <code>transform_profile(profile)</code> function that will add some noise to our data.\n",
    "\n",
    "Run the next cell to see the slightly noisy profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dea984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of using the dataclass and observing the dataset\n",
    "\n",
    "for phase in example_phase_offsets:\n",
    "    profile = data_class.get_interpolated_profile(phase)\n",
    "    noisy_profile = transform_profile(profile)\n",
    "    plt.figure()\n",
    "    plt.suptitle(f'Phase offset {phase}')\n",
    "    plt.subplot(121)\n",
    "    plt.title('Original simulated profile')\n",
    "    plt.plot(profile)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.title('Simulated profile w/ noise')\n",
    "\n",
    "    plt.plot(noisy_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab8af28",
   "metadata": {},
   "source": [
    "From these profiles, we can also calculate even more specific bunch characteristics.\n",
    "\n",
    "A function called <code>isolate_bunches_from_dm_profile</code> is provided to help you get the relative bunch lengths and intensities of your two bunches. It basically works by finding the center of each bunch, gathering a set number of bins around each center and defining this as your isolated bunch profile. In the process, it also outputs the FWHM and the intensity of your bunches.\n",
    "\n",
    "Run the cell below to see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a18ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do some analysis based on a profile, some additional utility functions are available\n",
    "from utils import profile_reward_quad, isolate_bunches_from_dm_profile\n",
    "\n",
    "profile = data_class.get_interpolated_profile(2)\n",
    "bunches, fwhms, intensities = isolate_bunches_from_dm_profile(profile, intensities=True, rel=True, plot_found_bunches=False)\n",
    "\n",
    "\n",
    "# Standardization/normalization\n",
    "fwhms = fwhms-np.mean(fwhms) # The FWHMs are returned normalized, but need to be centered around zero\n",
    "intensities = intensities / max(intensities) # The intensities are normalized, and centered around zero\n",
    "intensities = intensities - np.mean(intensities)\n",
    "\n",
    "### Plotting\n",
    "plt.figure()\n",
    "plt.subplot(131)\n",
    "plt.title('Full profile')\n",
    "plt.plot(profile)\n",
    "plt.subplot(132)\n",
    "plt.title('Isolated bunches')\n",
    "plt.plot(bunches[0],label='Bunch 1')\n",
    "plt.plot(bunches[1],label='Bunch 2')\n",
    "plt.legend()\n",
    "plt.subplot(133)\n",
    "plt.title('Relative BL and BI')\n",
    "plt.plot(fwhms, 'bo-', label='FWHMs')\n",
    "plt.plot(intensities, 'go-', label='Intensities')\n",
    "plt.axhline(0, color='k',linestyle='--')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587770ab",
   "metadata": {},
   "source": [
    "### Initial suggestion for state and difference estimate\n",
    "\n",
    "To simplify your initial attempt at an environment, I provide a function called <code>get_state_from_profile</code>, which will return a reasonable state to describe a double splitting using the final relative bunch lengths and intensities. This gives enough information to guide the agents next step, while also keeping your observation dimension small (which will decrease training time required).\n",
    "\n",
    "You can try the function below and will be expected to use it in your environment <code>_get_state</code> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce16214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_from_profile(profile):\n",
    "    \"\"\"Returns a state description of a double splitting given a final profile.\n",
    "\n",
    "    Args:\n",
    "        profile (numpy.ndarray): array or list-like object describing a profile of a double splitting.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A numpy array of four values, [bunch_length_1, bunch_length_2, bunch_intensity_1, bunch_intensity_2]\n",
    "    \"\"\"   \n",
    "    bunches, fwhms, intensities = isolate_bunches_from_dm_profile(profile, intensities=True, rel=True, plot_found_bunches=False)\n",
    "\n",
    "\n",
    "    # Standardization/normalization\n",
    "    fwhms = fwhms-np.mean(fwhms) # The FWHMs are returned normalized, but need to be centered around zero\n",
    "    intensities = intensities / max(intensities) # The intensities are normalized, and centered around zero\n",
    "    intensities = intensities - np.mean(intensities)\n",
    "    bls_and_intensities = np.append(fwhms, intensities)\n",
    "    return bls_and_intensities\n",
    "\n",
    "\n",
    "profile = data_class.get_interpolated_profile(2)\n",
    "\n",
    "state = get_state_from_profile(profile)\n",
    "\n",
    "print(f'State description: {state}, State type: {type(state)}')\n",
    "plt.figure()\n",
    "plt.title('Full profile')\n",
    "plt.plot(profile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f880a5e4",
   "metadata": {},
   "source": [
    "## Difference estimate\n",
    "\n",
    "To further ease your implemenation of an environment, a pre-defined function to calculate a difference estimate is provided. The function takes the state computed through <code>get_state_from_profile</code> and calculates a value based on the relative bunch lengths and intensities: the larger the value the bigger the differences between the bunches.\n",
    "\n",
    "The cell below defines the function for calculating the difference estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b44aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to scan your dataset using our difference estimate and reward to make sure it is sensible\n",
    "def get_diff_estimate_from_state(state):\n",
    "    \"\"\"\n",
    "    Returns an estimate descibing the difference between the final two bunches lengths and intensities\n",
    "    given the final relative bunch lengths and intensities of a double splitting (a state above).\n",
    "\n",
    "    Args:\n",
    "        state (numpy.ndarray): A numpy array of four values, [bunch_length_1, bunch_length_2, bunch_intensity_1, bunch_intensity_2]\n",
    "\n",
    "    Returns:\n",
    "        diff_estimate: A single value aiming to describe the difference between the final bunches.\n",
    "    \"\"\"   \n",
    "    observable = state\n",
    "    relative_bunch_lengths = observable[:2]\n",
    "    relative_bunch_intensities = observable[2:]\n",
    "    bunch_length_difference = abs(relative_bunch_lengths[0]-relative_bunch_lengths[1])\n",
    "    bunch_intensity_difference = abs(relative_bunch_intensities[0]-relative_bunch_intensities[1])\n",
    "\n",
    "    diff_estimate = bunch_intensity_difference + bunch_length_difference\n",
    "    \n",
    "    return diff_estimate\n",
    "\n",
    "min_phase=-45\n",
    "max_phase=45\n",
    "x = np.linspace(-45,45,361)\n",
    "diff_estimates = []\n",
    "plt.figure()\n",
    "for i in np.linspace(min_phase,max_phase,361):\n",
    "    profile = data_class.get_interpolated_profile(i)\n",
    "    state = get_state_from_profile(profile)\n",
    "    diff_estimate = get_diff_estimate_from_state(state)\n",
    "    diff_estimates.append(diff_estimate)\n",
    "    \n",
    "    \n",
    "plt.plot(x,diff_estimates, label='diff estimates')\n",
    "plt.axhline(0.02, color='k', linestyle='--', label='End criterion') # This is a pre-defined criterion giving a good splitting when using the diff_estimate.\n",
    "plt.legend()\n",
    "plt.ylabel('Difference estimates [arb. u.]')\n",
    "plt.xlabel('Phase offset [deg]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888a32d4",
   "metadata": {},
   "source": [
    "# Part 1 - Create your own environment\n",
    "\n",
    "In this part of the hands-on, you will be expected to implement your own environment and eventually train different types of RL agents in that environment. Don't worry, you will have a lot of guidance in the form of pre-defined functions and comments, but the goal is to provide you with enough knowledge to continue experimenting on your own after this session.\n",
    "\n",
    "## Reinforcement learning recap\n",
    "\n",
    "In the case of reinforcement learning, the agent learns through trial and error by interacting with an *environment* and receiving different *reward* signals. \n",
    "\n",
    "The environment supplies the agent with a state, describing the current situation of whatever problem the agent is tasked with solving. This state is given to the agent, which then makes an action affecting the environment and lands in a new state. An evaluation is run whether the action taken and the new state that we landed in was better or worse than the one before, and a *reward* is calculated. This reward describes how good the action was, and both the reward and new state are sent to the agent which then takes another action. This loop continues until the agent reaches some end criterion, which could for example be a number of steps or if the agent reaches a certain state. The goal of the agent during training is to maximize its total cumulative reward, which is often called the *return*. This general process is described in the agent-environment interaction loop.\n",
    "\n",
    "<img src=\"./images/agent-environment_2.PNG\" alt=\"rf program\" width=\"1000\"/>\n",
    "\n",
    "## Defining your environment\n",
    "\n",
    "Before we can train any cool RL agents, we need to define the environment it will interact with. To do this, we will leverage the very widely adopted OpenAI Gym framework https://www.gymlibrary.dev/.\n",
    "\n",
    "## The Gym - implements the agent-environment interaction loop\n",
    "\n",
    "The gym interface is simple, pythonic, and capable of representing general RL problems. It abstracts the interactions between an agent and an environment, and allows separation of the RL algorithm implementations from your custom environments. In essence, it implements the agent-environment interaction loop described above in a generalized manner.\n",
    "\n",
    "By defining only a few required functions, a user can create their very own environment to test their agents in, and thanks to the standardized methods other RL-libraries can often be used with these environments without any further changes required. An example library is Stable baselines3, which we will use for their implementations of different RL algorithms.\n",
    "\n",
    "Link to SB3 documentation: https://stable-baselines3.readthedocs.io/en/master/index.html\n",
    "\n",
    "A custom environment is created as a class that inherits the base class <code>gym.Env</code> from the <code>gym</code> package. An example of a skeleton environment is seen in the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e1549",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE OF CUSTOM ENVIRONMENT SKELETON CLASS ###\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "  \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "  metadata = {'render.modes': ['human']}\n",
    "\n",
    "  def __init__(self, arg1, arg2):\n",
    "    super(CustomEnv, self).__init__()\n",
    "    # Define action and observation space\n",
    "    # They must be gym.spaces objects!\n",
    "    \n",
    "    # Example with 2D action of value [-1,1]:\n",
    "    self.action_space = spaces.Box(\n",
    "                                  low=np.array([-1,-1]),\n",
    "                                  high=np.array([1,1]),\n",
    "                                  shape=(2,), dtype=np.float32)\n",
    "    # Example for using a vector of length 2 with values between [-1,1] as input:\n",
    "    self.observation_space = spaces.Box(low=np.array([-1,-1]),\n",
    "                                        high=np.array([1,1]),\n",
    "                                        shape=(2,), dtype=np.float32)\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"\n",
    "    The step function always takes (self, action) as input, and always returns (observation, reward, done, info).\n",
    "    Here, observation will connect to the latest state, reward is the latest returned reward (so a number), done: boolean, info: dictionary with information.\n",
    "    \"\"\"\n",
    "    ...\n",
    "    return observation, reward, done, info\n",
    "  def reset(self):\n",
    "    ...\n",
    "    return observation  # reward, done, info can't be included\n",
    "  def render(self, mode='human'):\n",
    "    ...\n",
    "  def close (self):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db14ef",
   "metadata": {},
   "source": [
    "In the code above you can see several functions that should be implemented if you want to use the basic gym interface. The minimum required functions are:\n",
    "\n",
    "- **Defining your action/observation space:** this will affect the input and output size of your agents (i.e. change the neural net architectures)\n",
    "- **The step() function:** Defines how your agent takes a step in your environment.\n",
    "- **The reset() function:** Will be called whenever an episode ends before starting the next episode.\n",
    "\n",
    "The render and close functions are not necessary, but can allow you to observe (or render) your environment to allow you to monitor your agents behaviour during training/testing and the close() function is to allow you to close your rendering window should this be required.\n",
    "\n",
    "In addition to these required functions, we will create some helper functions to use inside the base functions to structure our code more nicely, namely:\n",
    "\n",
    "- **_take_action(self, action):** Actually takes the action in the environment and updates the state. - You need to finish implementation!\n",
    "- **_get_state(self):** Get the observable given the current state we are in (evaluated from self.state) - You need to finish implementation! Hint: remember the <code>get_interpolated_profile()</code> and <code>get_state_from_profile()</code> functions from earlier.\n",
    "- **_get_reward(self):** Calculate the reward given the current state. - You need to finish implementation!\n",
    "\n",
    "\n",
    "Once we have all of these functions defined and set up in a class we will be able to train different kinds of RL agents in the same environment without the need to change almost anything. One does however need to make sure that the algorithm you want to use matches the action/observation spaces defined (for example continuous or discrete).\n",
    "\n",
    "## Implementing your environment\n",
    "\n",
    "We will begin by defining each of our important functions in a few assignments below, before finally compiling all of them together into a full environment class at the end.\n",
    "\n",
    "To ease the work on your side, I have left some skeleton codes for each assignment, but as you complete the earlier ones you will need to copy over your answers to the next code block in order to eventually build your complete environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e12953c",
   "metadata": {},
   "source": [
    "## 1.1 The observation/action spaces\n",
    "\n",
    "The first thing we need to define are our observation and action spaces. To conform to the gym standard, these spaces must be of a type present in <code>gym.spaces</code>.\n",
    "\n",
    "For this intial attempt, we will go with both a continous action and observation space. In order to keep the optimization of our agents efficient, we will also be using spaces normalized to be within the range -1,1 in each dimension. For these reasons, we will be using the <code>gym.spaces.Box()</code> type spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb4fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourDoubleEnv11(gym.Env):\n",
    "    \n",
    "    # Define your action and observation spaces. You want to take actions in one dimension, changing the h42 phase offset,\n",
    "    # and want a 4D observation, consisting of your relative bunch lengths and bunch intensities. Both your observations and \n",
    "    # actions will be should be normalized to be within [-1,1] for optimisation reasons.\n",
    "    \n",
    "    metadata= {'render.modes': ['human']}\n",
    "    \n",
    "    action_space = gym.spaces.Box(\n",
    "                                low = np.array([_]),# Fill in the _ with your lowest action. Dimensions of array correlate with dimensions of actions.\n",
    "                                high = np.array([_]),# Fill in the _ with your highest action.\n",
    "                                shape=(_,),           # Fill in the _ with the dimensions of your action space.\n",
    "                                dtype=np.float32)\n",
    "\n",
    "\n",
    "    ### Define what the observations are to be expected\n",
    "    observation_space = gym.spaces.Box(\n",
    "                                low = np.array([_]), # Fill in the __ with your lowest observations. This correlates to your state description!\n",
    "                                high = np.array([_]),# Fill in the __ with your highest observations.\n",
    "                                shape=(_,),\n",
    "                                dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627342fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import test_spaces\n",
    "\n",
    "test_environment = YourDoubleEnv11()\n",
    "print(test_environment.observation_space)\n",
    "print(test_environment.action_space)\n",
    "\n",
    "test_spaces(test_environment.observation_space, test_environment.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed97557",
   "metadata": {},
   "source": [
    "With our observation and action spaces defined, we can try to implement our step function. In order to achieve this, we will need to also implement the helper functions, <code>_take_action()</code>, and <code>_get_state()</code> and <code>_get_reward()</code>. \n",
    "\n",
    "## 1.2 Define _take_action() and _get_state() functions.\n",
    "\n",
    "In the cell below you will need to finish the implementation of the <code>_take_action()</code> function. The <code>_get_state()</code> function will also be implemented and then used in the <code>_take_action()</code> function.\n",
    "\n",
    "I also recommend you have a look at the <code>reset()</code> function. It is already implemented for you, but it is vital to get right if you try to write your own environment in the future. Think on: what needs to be reset in between episodes? How do we get an initial state? How do we reset our tracking lists?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some self variables that you could use in the real environment class\n",
    "class YourDoubleEnv12(gym.Env):\n",
    "    \n",
    "    # Define your action and observation spaces. You want to take actions in one dimension, changing the h42 phase offset,\n",
    "    # and want a 4D observation, consisting of your relative bunch lengths and bunch intensities. Both your observations and \n",
    "    # actions will be should be normalized to be within [-1,1] for optimisation reasons.\n",
    "    \n",
    "    metadata= {'render.modes': ['human']}\n",
    "    \n",
    "    ### INSERT YOUR ANSWER FROM 1.1. ###\n",
    "    \n",
    "    ### Define what the actions/observations are to be expected\n",
    "\n",
    "    action_space = \n",
    "    \n",
    "    observation_space = \n",
    "    \n",
    "    ### Stop criteria (constituting \"good\" splittings with provided difference estimate).\n",
    "    BUNCH_LENGTH_INT_CRITERIA = 0.02 # Empirically evaluated diff_estimate that constitutes a \"good\" bunch splitting. Lower means longer training time, but smaller spread in bunch lengths/intensities.\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def __init__(self,\n",
    "                max_steps = 100,\n",
    "                max_step_size = 20,\n",
    "                min_setting = -45,\n",
    "                max_setting = 45,):\n",
    "        \n",
    "        ### Assign hyperparameter settings to attributes\n",
    "        self.max_steps = max_steps\n",
    "        self.max_step_size = max_step_size\n",
    "        self.min_setting = min_setting\n",
    "        self.max_setting = max_setting\n",
    "        \n",
    "        ### Status of the iterations\n",
    "        # Steps, initializing lists to store actions/states/rewards...\n",
    "        self.step_counter = 0\n",
    "        self.curr_step = -1  ## Not used ?\n",
    "        self.phase_correction = 0\n",
    "        self.phase_set = 0\n",
    "\n",
    "        # Initialize lists for tracking episodes\n",
    "        self.curr_episode = -1\n",
    "        self.action_episode_memory = []\n",
    "        self.diff_estimate_memory = []\n",
    "        self.state_memory = []\n",
    "        self.phase_set_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.is_finalized = False\n",
    "    \n",
    "\n",
    "    def _take_action(self, action):\n",
    "        \"\"\"\n",
    "        Actual action funtion.\n",
    "\n",
    "        Action from model is scaled to be between [-1,1] for better optimization performance. \n",
    "        Converted back to phase setting in degrees using self.max_step_size.\n",
    "        \n",
    "        Args:\n",
    "            action (ndarray): n-dimensional action. Datatype, dimension, and value ranges defined in self.action_space.\n",
    "        \"\"\"\n",
    "        #######################################################################################################\n",
    "        # Implement the code below!!\n",
    "        #######################################################################################################\n",
    "\n",
    "        converted_action = __ # Convert your action from your normalized range [-1,1] back into degrees using self.max_step_size. HINT: action*self.max_step_size = what?\n",
    "        \n",
    "        # Phase offset as action, add offset to current phase_set to get next setting. This is what defines which simulated datapoint to collect in your _get_state function!!!\n",
    "        self.phase_set += __\n",
    "\n",
    "        # Update the self.state parameter with the new state. The preprovided self._get_state() will provide you with a state based on the current self.phase_set attribute value.\n",
    "        # The state will consist of a vector of 4 values: [bunch_length_1, bunch_length_2, bunch_intensity_1, bunch_intensity_2].\n",
    "        \n",
    "        self.state = self._get_state() # IMPLEMENT THIS FUNCTION!!\n",
    "\n",
    "\n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code above!!\n",
    "        ################################################################################################################################################################  \n",
    "        curr_state = self.state.copy()\n",
    "        curr_phase_set = np.copy(self.phase_set)\n",
    "        self.action_episode_memory.append(action)\n",
    "        self.state_memory.append(curr_state)\n",
    "        self.phase_set_memory.append(curr_phase_set)\n",
    "        self.phase_correction += converted_action # Phase correction tracks previous actions taken to get the cumulative change from start.        \n",
    "        self.step_counter += 1\n",
    "\n",
    "     \n",
    "    \n",
    "    def _get_state(self):\n",
    "        '''\n",
    "        Get the observable for a given phase_set. Use the self.phase_set\n",
    "\n",
    "        Comment: The edge cases of trying to move to datapoints outside the simulated dataset needs to be handled.\n",
    "        Currently it is simply checked whether the phase setting is above the max setting or below the min setting,\n",
    "        and if so a pre-defined dummy observation is presented. The important factor to consider is to make sure that\n",
    "        all edge cases are covered by some dummy state, and that the dummy states are unique (so the model can learn\n",
    "        what steps to take to get back in the right search area). It is also highly advised to give an additional penalty\n",
    "        in the reward if the agent steps outside our region of simulated data.\n",
    "        '''\n",
    "        \n",
    "        ### Check whether we are within simulated settings\n",
    "        if (self.phase_set<self.min_setting):\n",
    "            state = np.array([0.5, -0.5, 0.5, -0.5])\n",
    "        elif (self.phase_set>self.max_setting):\n",
    "            state = np.array([-0.5, 0.5, -0.5, 0.5])\n",
    "        else:\n",
    "            \n",
    "            ################################################################################################################################################################\n",
    "            # Implement the code below!!\n",
    "            #########################################################################################################################\n",
    "            \n",
    "            # You want to collect a profile corresponding to the current self.phase_set, \n",
    "            # and then calculate the state description.\n",
    "            profile = ___\n",
    "            state = ___\n",
    "            \n",
    "            # Hint: Remember the help functions for getting a profile from the dataset and turning a profile into a state.\n",
    "\n",
    "            ################################################################################################################################################################\n",
    "            # Implement the code above!!\n",
    "            ################################################################################################################################################################\n",
    "\n",
    "        return state.astype(np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset to a random state to start over a training episode.\n",
    "        This function will also be called everytime an episode is started\n",
    "        to provide an initial state.\n",
    "        \n",
    "        Returns:\n",
    "            state: The inital state of the environment. Should match the shape defined in observation_space.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Resetting to start a new episode\n",
    "        self.curr_episode += 1\n",
    "        self.step_counter = 0\n",
    "        self.is_finalized = False # This tracks wether an episode is complete or not.\n",
    "        \n",
    "        # Initializing episode lists to track data for individual episodes. Some used for rendering.\n",
    "        self.action_episode_memory = []\n",
    "        self.state_memory= []\n",
    "        self.phase_set_memory= []\n",
    "        self.reward_memory= []\n",
    "        self.diff_estimate_memory= []\n",
    "        \n",
    "        \n",
    "        # Getting initial state\n",
    "        \n",
    "        # Initialize random starting phase_set within [self.min_setting, self.max_setting], for example using random.uniform(min,max)\n",
    "        self.phase_set = random.uniform(self.min_setting,\n",
    "                                            self.max_setting)\n",
    "        \n",
    "        self.initial_offset = np.copy(self.phase_set)\n",
    "        self.phase_correction = 0\n",
    "                                        \n",
    "        self.state = self._get_state() # call _get_state to get the initial state from the starting phase_set.\n",
    "        state = self.state\n",
    "        \n",
    " \n",
    "        ### Some tracking of state, phase, reward, diff_estimate. Lets you use my render() function to observe your agent.\n",
    "        self.state_memory.append(state)\n",
    "        self.phase_set_memory.append(self.phase_set)\n",
    "        reward = self._get_reward()\n",
    "        self.reward_memory.append(reward)\n",
    "        \n",
    "        return state.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136ea55b",
   "metadata": {},
   "source": [
    "**Let's now test your <code>_take_action</code> method below...**\n",
    "\n",
    "The environment above will be initialized, reset to a starting setting using the provided reset() function, and then checked to make sure that the <code>_take_action</code> function updates the <code>self.phase_set</code> attribute.\n",
    "\n",
    "Two plots of the initial and final states will also be produced, check that these are different to make sure you also actually update the <code>self.state</code> attribute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8c1ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_environment = YourDoubleEnv12()\n",
    "test_environment.reset() # Reset env to a random starting phase_setting\n",
    "\n",
    "init_phase = test_environment.phase_set\n",
    "init_state = test_environment.state\n",
    "\n",
    "print(f'Initial phase_setting: {init_phase}')\n",
    "\n",
    "phase_action = 10\n",
    "normalized_action = phase_action/test_environment.max_step_size # Convert a 10 degree step into normalized action.\n",
    "test_environment._take_action(normalized_action)\n",
    "\n",
    "final_phase = test_environment.phase_set\n",
    "final_state = test_environment.state\n",
    "\n",
    "print(f'Final phase_setting after taking {phase_action} degree action: {test_environment.phase_set}')\n",
    "\n",
    "assert test_environment.phase_set == init_phase+phase_action\n",
    "print(' _take_action updated self.phase_set correctly. Good job!')\n",
    "\n",
    "plt.figure()\n",
    "ax1 = plt.subplot(121)\n",
    "plt.title(f'Initial state, phase {init_phase:.2f}')\n",
    "plt.plot(init_state[:2], 'o-', label = 'Rel. bunch lengths')\n",
    "plt.plot(init_state[2:], 'o-', label = 'Rel. bunch intensities')\n",
    "plt.legend()\n",
    "ax2 = plt.subplot(122, sharey=ax1)\n",
    "plt.title(f'Final state, phase {final_phase:.2f}')\n",
    "plt.plot(final_state[:2], 'o-', label = 'Rel. bunch lengths')\n",
    "plt.plot(final_state[2:], 'o-', label = 'Rel. bunch intensities')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d9518",
   "metadata": {},
   "source": [
    "## 1.3 Define _get_reward() function.\n",
    "\n",
    "The <code>_get_reward()</code> function will be called after we have taken an action, to calculate a reward based on the state we ended up in. This is the second function we will need to assemble our <code>step()</code> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some self variables that you could use in the real environment class\n",
    "class YourDoubleEnv13(gym.Env):\n",
    "    \n",
    "    # Define your action and observation spaces. You want to take actions in one dimension, changing the h42 phase offset,\n",
    "    # and want a 4D observation, consisting of your relative bunch lengths and bunch intensities. Both your observations and \n",
    "    # actions will be should be normalized to be within [-1,1] for optimisation reasons.\n",
    "    \n",
    "    metadata= {'render.modes': ['human']}\n",
    "    \n",
    "    ### INSERT YOUR ANSWER FROM 1.1. ###\n",
    "    action_space = \n",
    "\n",
    "    ### Define what the observations are to be expected\n",
    "    observation_space = \n",
    "    \n",
    "    ### Stop criteria (constituting \"good\" splittings with provided difference estimate).\n",
    "    BUNCH_LENGTH_INT_CRITERIA = 0.02 # Empirically evaluated diff_estimate that constitutes a \"good\" bunch splitting. Lower means longer training time, but smaller spread in bunch lengths/intensities.\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def __init__(self,\n",
    "                max_steps = 100,\n",
    "                max_step_size = 20,\n",
    "                min_setting = -45,\n",
    "                max_setting = 45,):\n",
    "        \n",
    "        ### Assign hyperparameter settings to attributes\n",
    "        self.max_steps = max_steps\n",
    "        self.max_step_size = max_step_size\n",
    "        self.min_setting = min_setting\n",
    "        self.max_setting = max_setting\n",
    "        \n",
    "        ### Status of the iterations\n",
    "        # Steps, initializing lists to store actions/states/rewards...\n",
    "        self.step_counter = 0\n",
    "        self.curr_step = -1  ## Not used ?\n",
    "        self.phase_correction = 0\n",
    "        self.phase_set = 0\n",
    "\n",
    "        # Initialize lists for tracking episodes\n",
    "        self.curr_episode = -1\n",
    "        self.action_episode_memory = []\n",
    "        self.diff_estimate_memory = []\n",
    "        self.state_memory = []\n",
    "        self.phase_set_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.is_finalized = False\n",
    "    \n",
    "\n",
    "    def _take_action(self, action):\n",
    "        \"\"\"\n",
    "        Actual action funtion.\n",
    "\n",
    "        Action from model is scaled to be between [-1,1] for better optimization performance. \n",
    "        Converted back to phase setting in degrees using self.max_step_size.\n",
    "        \n",
    "        Args:\n",
    "            action (ndarray): n-dimensional action. Datatype, dimension, and value ranges defined in self.action_space.\n",
    "        \"\"\"\n",
    "        ################################################################################################################################################################\n",
    "        # Insert answer from 1.2!!\n",
    "        ################################################################################################################################################################\n",
    "\n",
    "        converted_action = ______# Convert your action from your normalized range [-1,1] back into degrees using self.max_step_size. HINT: action*self.max_step_size = what?\n",
    "        self.phase_correction += converted_action # Phase correction tracks previous actions taken to get the cumulative change from start.\n",
    "\n",
    "        # Phase offset as action, add offset to current phase_set to get next setting. This is what defines which simulated datapoint to collect in your _get_state function!!!\n",
    "        self.phase_set += converted_action\n",
    "\n",
    "        # Update the self.state parameter with the new state. The preprovided self._get_state() will provide you with a state based on the current self.phase_set attribute value.\n",
    "        # The state will consist of a vector of 4 values: [bunch_length_1, bunch_length_2, bunch_intensity_1, bunch_intensity_2].\n",
    "        self.state = self._get_state()\n",
    "\n",
    "\n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code above!!\n",
    "        ################################################################################################################################################################  \n",
    "        curr_state = self.state.copy()\n",
    "        curr_phase_set = np.copy(self.phase_set)\n",
    "        self.action_episode_memory.append(action)\n",
    "        self.state_memory.append(curr_state)\n",
    "        self.phase_set_memory.append(curr_phase_set)\n",
    "        \n",
    "        self.step_counter += 1\n",
    "\n",
    "     \n",
    "    \n",
    "    def _get_state(self):\n",
    "        '''\n",
    "        Get the observable for a given phase_set.\n",
    "\n",
    "        Comment: The edge cases of trying to move to datapoints outside the simulated dataset needs to be handled.\n",
    "        Currently it is simply checked whether the phase setting is above the max setting or below the min setting,\n",
    "        and if so a pre-defined dummy observation is presented. The important factor to consider is to make sure that\n",
    "        all edge cases are covered by some dummy state, and that the dummy states are unique (so the model can learn\n",
    "        what steps to take to get back in the right search area). It is also highly advised to give an additional penalty\n",
    "        in the reward if the agent steps outside our region of simulated data.\n",
    "        '''\n",
    "        \n",
    "        ### Check whether we are within simulated settings\n",
    "        if (self.phase_set<self.min_setting):\n",
    "            state = np.array([0.5, -0.5, 0.5, -0.5])\n",
    "        elif (self.phase_set>self.max_setting):\n",
    "            state = np.array([-0.5, 0.5, -0.5, 0.5])\n",
    "        else:\n",
    "            \n",
    "            ################################################################################################################################################################\n",
    "            # Insert answer from 1.2!!\n",
    "            #########################################################################################################################\n",
    "            \n",
    "            \n",
    "            # Hint: Remember the help functions for getting a profile from the dataset and turning a profile into a state.\n",
    "\n",
    "            ################################################################################################################################################################\n",
    "            # Implement the code above!!\n",
    "            ################################################################################################################################################################\n",
    "\n",
    "        return state.astype(np.float32)\n",
    "    \n",
    "    def _get_reward(self):\n",
    "        \"\"\" Evaluating the reward from the observable/state.  \n",
    "\n",
    "        Returns:\n",
    "            float: The reward based on the current state. \n",
    "        \"\"\" \n",
    "        \n",
    "        # The shape of your observable should match your optimization_space. For this excercise, it is expected\n",
    "        # that you use an observation space of a vector with four values\n",
    "        observable = self.state\n",
    "\n",
    "        diff_estimate = get_diff_estimate_from_state(observable)\n",
    "        self.diff_estimate = diff_estimate\n",
    "\n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code below!!\n",
    "        ################################################################################################################################################################\n",
    "        \"\"\" \n",
    "         Define your own reward here. The diff_estimate provided above is provided \n",
    "         as a simple way to define the difference between your bunches length/intensity \n",
    "         after the splitting. You want this to be as small as possible, so a better reward\n",
    "         should be given for a smaller diff_estimate. There is already a pre-defined\n",
    "         criterion for the diff_estimate to be considered a \"good\" splitting provided in\n",
    "         the BUNCH_LENGTH_INT_CRITERIA attribute. \n",
    "         \"\"\"\n",
    "\n",
    "        reward = ______ # Define your own reward here!! \n",
    "\n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code above!!\n",
    "        ################################################################################################################################################################\n",
    "        \n",
    "        ### Tracking of diff_estimate. Lets you use my render() function to observe your agent.\n",
    "        curr_diff_estimate = self.diff_estimate.copy()\n",
    "        self.diff_estimate_memory.append(curr_diff_estimate)\n",
    "        self.reward_memory.append(reward)\n",
    "            \n",
    "        return reward\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset to a random state to start over a training episode.\n",
    "        This function will be called everytime an episode is started\n",
    "        to provide an initial state.\n",
    "        \n",
    "        Returns:\n",
    "            state: The inital state of the environment. Should match the shape defined in observation_space.\n",
    "        \"\"\"\n",
    "        # Resetting to start a new episode\n",
    "        self.curr_episode += 1\n",
    "        self.step_counter = 0\n",
    "        self.is_finalized = False # This tracks wether an episode is complete or not.\n",
    "        \n",
    "        #print(f'Resetting for episode {self.curr_episode}')\n",
    "\n",
    "        # Initializing episode lists to track data for individual episodes. Some used for rendering.\n",
    "        self.action_episode_memory = []\n",
    "        self.state_memory= []\n",
    "        self.phase_set_memory= []\n",
    "        self.reward_memory= []\n",
    "        self.diff_estimate_memory= []\n",
    "        \n",
    "       \n",
    "        # Getting initial state\n",
    "        \n",
    "        # Initialize random starting phase_set within [self.min_setting, self.max_setting], for example using random.uniform(min,max)\n",
    "        self.phase_set = random.uniform(self.min_setting,\n",
    "                                            self.max_setting)\n",
    "        \n",
    "        self.initial_offset = np.copy(self.phase_set)\n",
    "        self.phase_correction = 0\n",
    "                                        \n",
    "        self.state = self._get_state() # call _get_state to get the initial state from the starting phase_set.\n",
    "        state = self.state\n",
    "\n",
    "        ### Some tracking of state, phase, reward, diff_estimate. Lets you use my render() function to observe your agent.\n",
    "        self.state_memory.append(state)\n",
    "        self.phase_set_memory.append(self.phase_set)\n",
    "        reward = self._get_reward()\n",
    "        self.reward_memory.append(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b4486",
   "metadata": {},
   "source": [
    "**Testing the <code>_get_reward</code> function...**\n",
    "\n",
    "Simply run the cell below to produce some plots comparing the difference estimate with the reward given in the same phase offset setting. Make sure it seems reasonable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_environment = YourDoubleEnv13()\n",
    "\n",
    "from utils import test_reward\n",
    "# Set the environment to different states in our observation space, and check the value of the reward.\n",
    "# We will also plot our difference estimate, to see that our reward are reasonable in relation to our\n",
    "# bunch characteristics.\n",
    "\n",
    "test_reward(test_environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2758f03",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Assuming you are happy with your reward shape, you are now ready to assemble the main function of your environment: the <code>step()</code> function.\n",
    "\n",
    "## 1.4 Define the step() function\n",
    "\n",
    "We will define the <code>step()</code> function with the helper functions defined in 1.2-1.3. Both the input and the output of the function are pre-defined according to the <code>gym</code> standard.\n",
    "\n",
    "- **Input**: self (the class object and its attributes), action (the action you want to take in the environment).\n",
    "- **Output**: 4-tuple of state, reward, is_finalized, info\n",
    "    - **state**: the end state after taking a step\n",
    "    - **reward**: the reward given after taking the step and landing in state\n",
    "    - **is_finalized**: boolean that is true if the state is terminal, false otherwise.\n",
    "    - **info**: dictionary where it is possible to return any extra information you may be interested in (e.g. for debugging, plotting...)\n",
    "    \n",
    "Below you will find a class where a skeleton for the <code>step()</code> function is given. It uses the functions and spaces from excercises 1.1-1.3, so copy them down into their corresponding locations.\n",
    "    \n",
    "To help you in the implementation, the following pseudocode is provided:\n",
    "\n",
    "<pre><code>def step(self, action):</code>\n",
    "    \n",
    "    1. Take action *action*\n",
    "    2. Get the reward given by following *state*.\n",
    "    3. Check exit criterions\n",
    "\n",
    "        3.1 Is the self.diff_estimate < BUNCH_LENGTH_INT_CRITERIA?\n",
    "            Yes: Episode is_finalized = True, success = True.\n",
    "            No: pass\n",
    "\n",
    "        3.2 Are our number_of_steps > max_steps?\n",
    "            Yes: Episode is_finalized = True, success = False.\n",
    "            No: pass\n",
    "\n",
    "    return state, reward, is_finalized, info </pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f58c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some self variables that you could use in the real environment class\n",
    "class YourDoubleEnv(gym.Env):\n",
    "    \n",
    "    # Define your action and observation spaces. You want to take actions in one dimension, changing the h42 phase offset,\n",
    "    # and want a 4D observation, consisting of your relative bunch lengths and bunch intensities. Both your observations and \n",
    "    # actions will be should be normalized to be within [-1,1] for optimisation reasons.\n",
    "    \n",
    "    metadata= {'render.modes': ['human']}\n",
    "    \n",
    "    ### Insert answer from 1.1\n",
    "    action_space = \n",
    "\n",
    "\n",
    "    ### Define what the observations are to be expected\n",
    "    observation_space = \n",
    "    \n",
    "    ### Stop criteria (constituting \"good\" splittings with provided difference estimate).\n",
    "    BUNCH_LENGTH_INT_CRITERIA = 0.02 # Empirically evaluated diff_estimate that constitutes a \"good\" bunch splitting. Lower means longer training time, but smaller spread in bunch lengths/intensities.\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def __init__(self,\n",
    "                max_steps = 100,\n",
    "                max_step_size = 20,\n",
    "                min_setting = -45,\n",
    "                max_setting = 45,):\n",
    "        \n",
    "        ### Assign hyperparameter settings to attributes\n",
    "        self.max_steps = max_steps\n",
    "        self.max_step_size = max_step_size\n",
    "        self.min_setting = min_setting\n",
    "        self.max_setting = max_setting\n",
    "        \n",
    "        ### Status of the iterations\n",
    "        # Steps, initializing lists to store actions/states/rewards...\n",
    "        \n",
    "        self.step_counter = 0 # Counts the number of steps taken in the environment!\n",
    "        self.curr_step = -1  ## Not used ?\n",
    "        self.phase_correction = 0\n",
    "\n",
    "        # Initialize lists for tracking episodes\n",
    "        self.curr_episode = -1\n",
    "        self.action_episode_memory = []\n",
    "        self.diff_estimate_memory = []\n",
    "        self.state_memory = []\n",
    "        self.phase_set_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.is_finalized = False\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        One step/action in the environment, returning the observable\n",
    "        and reward. \n",
    "\n",
    "        Stopping conditions: max_steps reached, or splitting good enough.\n",
    "        \"\"\"\n",
    "        success = False    \n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code below!!\n",
    "        ################################################################################################################################################################\n",
    "                \n",
    "    \n",
    "        \n",
    "\n",
    "        # Here you can add any extra info you would like to be returned on each step, e.g. episode steps, rewards, actions etc.\n",
    "        info = {'success': success, 'steps': self.step_counter, 'profile': self.profile} \n",
    "        \n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code above!!\n",
    "        ################################################################################################################################################################\n",
    "        state = self.state\n",
    "        \n",
    "        return state, reward.astype(np.float64), self.is_finalized, info # Standardized output according to gym framework.\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        \"\"\"\n",
    "        Actual action funtion.\n",
    "\n",
    "        Action from model is scaled to be between [-1,1] for better optimization performance. \n",
    "        Converted back to phase setting in degrees using self.max_step_size.\n",
    "        \n",
    "        Args:\n",
    "            action (ndarray): n-dimensional action. Datatype, dimension, and value ranges defined in self.action_space.\n",
    "        \"\"\"\n",
    "        ################################################################################################################################################################\n",
    "        # Insert from 1.2\n",
    "        ################################################################################################################################################################\n",
    "\n",
    "        converted_action = action*self.max_step_size # Convert your action from your normalized range [-1,1] back into degrees using self.max_step_size. HINT: action*self.max_step_size = what?\n",
    "        self.phase_correction += converted_action # Phase correction tracks previous actions taken to get the cumulative change from start.\n",
    "\n",
    "        # Phase offset as action, add offset to current phase_set to get next setting. This is what defines which simulated datapoint to collect in your _get_state function!!!\n",
    "        self.phase_set += converted_action\n",
    "\n",
    "        # Update the self.state parameter with the new state. The preprovided self._get_state() will provide you with a state based on the current self.phase_set attribute value.\n",
    "        # The state will consist of a vector of 4 values: [bunch_length_1, bunch_length_2, bunch_intensity_1, bunch_intensity_2].\n",
    "        self.state = self._get_state()\n",
    "\n",
    "\n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code above!!\n",
    "        ################################################################################################################################################################  \n",
    "        curr_state = self.state.copy()\n",
    "        curr_phase_set = self.phase_set.copy()\n",
    "        self.action_episode_memory.append(action)\n",
    "        self.state_memory.append(curr_state)\n",
    "        self.phase_set_memory.append(curr_phase_set)\n",
    "        \n",
    "        self.step_counter += 1\n",
    "\n",
    "     \n",
    "    \n",
    "    def _get_state(self):\n",
    "        '''\n",
    "        Get the observable for a given phase_set. \n",
    "\n",
    "        Comment: The edge cases of trying to move to datapoints outside the simulated dataset needs to be handled.\n",
    "        Currently it is simply checked whether the phase setting is above the max setting or below the min setting,\n",
    "        and if so a pre-defined dummy observation is presented. The important factor to consider is to make sure that\n",
    "        all edge cases are covered by some dummy state, and that the dummy states are unique (so the model can learn\n",
    "        what steps to take to get back in the right search area). It is also highly advised to give an additional penalty\n",
    "        in the reward if the agent steps outside our region of simulated data.\n",
    "        '''\n",
    "        \n",
    "        ### Check whether we are within simulated settings\n",
    "        if (self.phase_set<self.min_setting):\n",
    "            state = np.array([0.5, -0.5, 0.5, -0.5])\n",
    "        elif (self.phase_set>self.max_setting):\n",
    "            state = np.array([-0.5, 0.5, -0.5, 0.5])\n",
    "        else:\n",
    "            \n",
    "            ################################################################################################################################################################\n",
    "            # Insert from 1.2!!\n",
    "            #########################################################################################################################\n",
    "            \n",
    "            \n",
    "            \n",
    "            ################################################################################################################################################################\n",
    "            # Implement the code above!!\n",
    "            ################################################################################################################################################################\n",
    "        \n",
    "            self.profile = profile # Add a tracking of the profile for plotting purposes!\n",
    "        return state.astype(np.float32)\n",
    "    \n",
    "    def _get_reward(self):\n",
    "        \"\"\" Evaluating the reward from the observable/state. \n",
    "            \n",
    "\n",
    "        Returns:\n",
    "            float: The reward based on the current state. \n",
    "        \"\"\" \n",
    "        \n",
    "        \n",
    "        \n",
    "        # The shape of your observable should match your optimization_space. For this excercise, it is expected\n",
    "        # that you use an observation space of a vector with four values\n",
    "        observable = self.state\n",
    "\n",
    "        diff_estimate = get_diff_estimate_from_state(observable)\n",
    "        self.diff_estimate = diff_estimate\n",
    "\n",
    "        ################################################################################################################################################################\n",
    "        # Insert from 1.3!!\n",
    "        ################################################################################################################################################################\n",
    "        \"\"\" \n",
    "         Define your own reward here. The diff_estimate provided above is provided \n",
    "         as a simple way to define the difference between your bunches length/intensity \n",
    "         after the splitting. You want this to be as small as possible, so a better reward\n",
    "         should be given for a smaller diff_estimate. There is already a pre-defined\n",
    "         criterion for the diff_estimate to be considered a \"good\" splitting provided in\n",
    "         the BUNCH_LENGTH_INT_CRITERIA attribute. \n",
    "         \"\"\"\n",
    "\n",
    "        reward = ______ # Define your own reward here!! \n",
    "\n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code above!!\n",
    "        ################################################################################################################################################################\n",
    "        \n",
    "        ### Tracking of diff_estimate. Lets you use my render() function to observe your agent.\n",
    "        curr_diff_estimate = self.diff_estimate.copy()\n",
    "        self.diff_estimate_memory.append(curr_diff_estimate)\n",
    "        self.reward_memory.append(reward)\n",
    "            \n",
    "        return reward\n",
    "       \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset to a random state to start over a training episode.\n",
    "        This function will be called everytime an episode is started\n",
    "        to provide an initial state.\n",
    "        \n",
    "        Returns:\n",
    "            state: The inital state of the environment. Should match the shape defined in observation_space.\n",
    "        \"\"\"\n",
    "        # Resetting to start a new episode\n",
    "        self.curr_episode += 1\n",
    "        self.step_counter = 0\n",
    "        self.is_finalized = False # This tracks wether an episode is complete or not.\n",
    "        \n",
    "        #print(f'Resetting for episode {self.curr_episode}')\n",
    "\n",
    "        # Initializing episode lists to track data for individual episodes. Some used for rendering.\n",
    "        self.action_episode_memory = []\n",
    "        self.state_memory= []\n",
    "        self.phase_set_memory= []\n",
    "        self.reward_memory= []\n",
    "        self.diff_estimate_memory= []\n",
    "        \n",
    "       \n",
    "        \n",
    "        # Getting initial state\n",
    "        \n",
    "        # Initialize random starting phase_set within [self.min_setting, self.max_setting], for example using random.uniform(min,max)\n",
    "        self.phase_set = random.uniform(self.min_setting,\n",
    "                                            self.max_setting)\n",
    "        \n",
    "        self.initial_offset = np.copy(self.phase_set)\n",
    "        self.phase_correction = 0\n",
    "                                        \n",
    "        self.state = self._get_state() # call _get_state to get the initial state from the starting phase_set.\n",
    "        state = self.state\n",
    " \n",
    "        ### Some tracking of state, phase, reward, diff_estimate. Lets you use my render() function to observe your agent.\n",
    "        self.state_memory.append(state)\n",
    "        self.phase_set_memory.append(self.phase_set)\n",
    "        reward = self._get_reward()\n",
    "        self.reward_memory.append(reward)\n",
    "        \n",
    "        return state.astype(np.float32)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"\n",
    "        Set the random seed. Useful if you want to standardize trainings.\n",
    "        \"\"\"\n",
    "        \n",
    "        random.seed(seed)\n",
    "        np.random.seed\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Rendering function meant to provide a human-readable output. Base function in gym\n",
    "        environments to override. I provide a simple version that should let you observe \n",
    "        your trained agent during evaluation.\n",
    "        \"\"\"\n",
    "        plt.figure('Agent')\n",
    "        plt.clf()\n",
    "        plt.subplot(131)\n",
    "        plt.suptitle(f'Episode {self.curr_episode}')\n",
    "        plt.title('Current profile')\n",
    "        plt.plot(self.profile,'b')\n",
    "        plt.subplot(132)\n",
    "        plt.title('Difference estimate')\n",
    "        plt.plot(self.diff_estimate_memory, 'o-')\n",
    "        plt.axhline(y=self.BUNCH_LENGTH_INT_CRITERIA, color='k', linestyle='--')\n",
    "        plt.subplot(133)\n",
    "        plt.title('h42 phase offset')\n",
    "        plt.plot(np.asarray(self.phase_set_memory, dtype=object), 'go-')\n",
    "        plt.axhline(y=0, color='k', linestyle='--')\n",
    "        plt.ylim((-30,30))\n",
    "        \n",
    "        #plot_finish(fig=fig, axes=axes, xlabel='Setting', ylabel='Observable')\n",
    "        plt.pause(0.2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322c7cf",
   "metadata": {},
   "source": [
    "With the environment above completed and implemented, our environment is complete! As a final test, we can use an environment checker provided by stable-baselines3 to check whether our environment truly follows the standards of gym and SB3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "env = YourDoubleEnv()\n",
    "\n",
    "result = check_env(env)\n",
    "\n",
    "if result==None:\n",
    "    print(f'Check env succesful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec9d2b2",
   "metadata": {},
   "source": [
    "# Part 2: Training RL-agents in your environment\n",
    "\n",
    "Now that your environment is ready, it is time to train some agents! We will focus on the deep RL methods implemented in Stable Baselines 3, as they are generally compatible with a generic gym evnironment and very powerful for solving problems without the need of modeling (the algorithms we will use today are all *model-free*).\n",
    "\n",
    "We will start with one of the most prominent actor-critic methods: Soft Actor Critic (SAC).\n",
    "\n",
    "Small note: from this section forward, no more coding is necessary to run the cells. However, after going through the training once feel free to go through and experiment with the different hyperparameters of the algorithms, or to go back to your environment and redefine your rewards/state descriptions.\n",
    "\n",
    "\n",
    "## 2.1 Soft Actor Critic\n",
    "\n",
    "Soft Actor critic is a model-free, off-policy, actor-critic algorithm that optimizes a stochastic policy. In essence, this means it is easy to apply to new problems, and learns simply through it's experiences with no need for a previous policy or model to be declared.\n",
    "\n",
    "The algorithm is also known for being relatively stable, with little need for hyperparameter tuning (an otherwise common issue in RL). Due to these reasons, it has become a popular algorithms for a variety of tasks.\n",
    "\n",
    "\n",
    "You can read a summarized introduction to the algorithm at OpenAI SpinningUp: https://spinningup.openai.com/en/latest/algorithms/sac.html#soft-actor-critic\n",
    "\n",
    "or see the original paper: https://arxiv.org/pdf/1801.01290.pdf\n",
    "\n",
    "And the Stable Baselines3 documentation: https://stable-baselines3.readthedocs.io/en/master/modules/sac.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ef9998",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell is already complete and should run without any changes necessary.\n",
    "### However, it does assume that the environment defined in previous assignment\n",
    "### follows the gym API.\n",
    "\n",
    "# Train a model\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "\n",
    "\n",
    "# Separate evaluation env\n",
    "eval_env = YourDoubleEnv()\n",
    "\n",
    "# Use deterministic actions for evaluation\n",
    "eval_callback = EvalCallback(eval_env,  best_model_save_path='./RL_logs/SAC', # Define a callback to evaluate our model \n",
    "    log_path='./RL_logs/', eval_freq=200,                                     # periodically. Also saves the best evaluated\n",
    "    deterministic=True, render=False)                                         # model to ./RL_logs/SAC/best_model.zip.\n",
    "\n",
    "\n",
    "env = YourDoubleEnv()\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "log_dir = './'\n",
    "\n",
    "\n",
    "ent_coef = 1/5#'auto' # Hyperparameter of SAC, with 'auto' it is also learned during training.\n",
    "\n",
    "# Define your model\n",
    "sac_model = SAC(\"MlpPolicy\", env, verbose=1, ent_coef=ent_coef, learning_starts=100, tensorboard_log=\"./hands-on_rl_tensorboard\")\n",
    "print(\"Starting training...\")\n",
    "sac_model.learn(total_timesteps=100000, callback=eval_callback, log_interval=1, tb_log_name=\"hands_on_rl-SAC\")\n",
    "sac_model.save('./saved_models/RL_agent') # Save our final model\n",
    "print(\"Completed training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44ee07",
   "metadata": {},
   "source": [
    "**Monitoring training through Tensorboard**\n",
    "\n",
    "If you want to track the training above and also see the loss curves of your model, you can use Tensorboard. In a normal jupyter notebook you should be able to run tensorboard directly using \n",
    "\n",
    "<code>\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir ./RL_logs\n",
    "</code>\n",
    "\n",
    "You may need to run the command <code>%reload_ext tensorboard</code> occasionally to update the tensorboard data.\n",
    "\n",
    "I recommend you try this after training to be able to see interesting metrics of your agent and how they evolve with training!\n",
    "\n",
    "\n",
    "\n",
    "In the tensorboard graphs, you can see how your agent evolves over time. If you look under the tab \"rollout\", you will be able to see two graphs: **ep_len_mean** and **ep_rew_mean**, who stand for mean episode length and mean episode reward, respectively. Here you will be able to track how your agent is performing on average during training, with the timesteps on the x-axis. Of course you want the mean reward to be as large as possible, and the episode length to be as small as possible.\n",
    "\n",
    "\n",
    "\n",
    "You can try and see if you understand the other tensorboard graphs on your own, otherwise feel free to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810cfb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2d210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reload_ext tensorboard\n",
    "%tensorboard --logdir ./hands-on_rl_tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ca3c2c",
   "metadata": {},
   "source": [
    "If you are having trouble training your agent, to let you run the testing below you can load an SAC agent trained by me for 1000 timesteps in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62f9cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading 1000 timesteps SAC model\n",
    "\n",
    "sac_model = SAC.load('./saved_models/backup/SAC_RL_agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0636b1b1",
   "metadata": {},
   "source": [
    "## Test your trained agent\n",
    "\n",
    "Now you should have a simple agent trained, after taking 1000 steps in the environment. Now let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9867a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trained_agent(environment, agent, test_episodes_to_run = 10, render_opt=False):\n",
    "    obs = environment.reset()\n",
    "    performance = [0,0]\n",
    "    steps_per_episode = []\n",
    "    predicted_phase_error_when_done = []\n",
    "    done=False\n",
    "\n",
    "    start_diff_estimates = []\n",
    "    end_diff_estimates = []\n",
    "    initial_profiles = []\n",
    "    final_profiles = []\n",
    "    counter = 0\n",
    "    episode = 0\n",
    "    plt.figure('Agent')\n",
    "    for episode in range(test_episodes_to_run):\n",
    "        done=False\n",
    "        first_step = True\n",
    "        #print(obs)\n",
    "        while not done:\n",
    "            action, _states = agent.predict(obs, deterministic=True)\n",
    "            #print(f' action {action}')\n",
    "            obs, rewards, done, info = test_env.step(action)\n",
    "            if render_opt:\n",
    "                test_env.render() # Use if you want to observe the agent\n",
    "                if done:\n",
    "                    time.sleep(1) # allow some time to see complete trajectory before starting next episode.\n",
    "            if done:\n",
    "\n",
    "                print(f\"Took {info['steps']} steps before terminating test\")\n",
    "                steps_per_episode.append(info['steps'])\n",
    "                #print(f\"Info {info}\")\n",
    "                \n",
    "                if info['success'] == True:\n",
    "                    performance[0] += 1\n",
    "                else:\n",
    "                    performance[1] += 1\n",
    "\n",
    "                if counter < 5:\n",
    "                    final_profiles.append(info['profile'])\n",
    "                counter += 1\n",
    "                obs = test_env.reset()\n",
    "    print(f\"Succesful optimizations: {performance[0]}\")\n",
    "    print(f\"Unsuccesful optimizations: {performance[1]}\")\n",
    "    print(f\"Accuracy: {performance[0]/(performance[0]+performance[1])*100}%\")\n",
    "    print(f\"Mean episode length: {np.sum(steps_per_episode)/len(steps_per_episode)}\")\n",
    "    print(f\"Max episode length: {np.max(steps_per_episode)}, Min episode length: {np.min(steps_per_episode)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdf7f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "# Reset environment\n",
    "# Create new test env\n",
    "test_env = YourDoubleEnv()\n",
    "\n",
    "\n",
    "test_trained_agent(test_env, sac_model, render_opt = True) # If you want to test for more episodes, you set render_opt=False\n",
    "                                                           # and add an input test_episodes_to_run = 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc49f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively to running with the sac_model stored in memory, we can load our best model \n",
    "# evaluated during testing.\n",
    "\n",
    "sac_model_loaded_best = SAC.load('./RL_logs/SAC/best_model')\n",
    "test_env = YourDoubleEnv()\n",
    "test_trained_agent(test_env, sac_model_loaded_best, render_opt = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dfe957",
   "metadata": {},
   "source": [
    "## How did your agent work?\n",
    "\n",
    "Did it manage to solve the problem during testing? What if you train the agent for a longer duration? Feel free to experiment with the hyperparameters of your training until you get an SAC-agent you are happy with.\n",
    "\n",
    "- How fast is your best agent at optimizing the splitting?\n",
    "- Think about what limitations there are on the optimization efficiency. What is needed to allow the agent a (theoretical) one-step optimization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7386b23a",
   "metadata": {},
   "source": [
    "## Compare with pre-trained agent\n",
    "\n",
    "As an example of a well trained agent, you can load the super_agent below and test it in an environment, to see the efficiency that you can get out of an SAC agent with this problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708f075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_model = SAC.load('./RL_logs/SAC/Super_lims_45/best_model')\n",
    "test_env = YourDoubleEnv(max_step_size=45)\n",
    "test_trained_agent(test_env, super_model, render_opt = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e07c57",
   "metadata": {},
   "source": [
    "## Versatility of your Gym environment: Training other agents (other algorithms)\n",
    "\n",
    "Now, to illustrate the utility of the gym framework, we will also train another RL agent using a different algorithm: TD3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf34d92",
   "metadata": {},
   "source": [
    "## 2.2 Twin Delayed Deep Deterministic Policy Gradient (TD3)\n",
    "\n",
    "Similar to SAC (and developed around the same time) TD3 is also a model-free off-policy RL algorithm. It improves upon the older DDPG agent and is based on so-called policy gradient methods. In contrast with SAC, TD3 always produces a deterministic policy after training.\n",
    "\n",
    "Spinning Up: https://spinningup.openai.com/en/latest/algorithms/td3.html\n",
    "\n",
    "Stable Baselines3 docs: https://stable-baselines3.readthedocs.io/en/master/modules/td3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf128b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "\n",
    "# Separate evaluation env\n",
    "eval_env = YourDoubleEnv()\n",
    "\n",
    "# Use deterministic actions for evaluation\n",
    "eval_callback = EvalCallback(eval_env,  best_model_save_path='./RL_logs/TD3',\n",
    "    log_path='./RL_logs/', eval_freq=100,\n",
    "    deterministic=True, render=False)\n",
    "\n",
    "\n",
    "env = YourDoubleEnv()\n",
    "\n",
    "# The noise objects for TD3. Injects noise in the actions to encourage exploration.\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "log_dir = './'\n",
    "\n",
    "\n",
    "\n",
    "# Define your model\n",
    "td3_model = TD3(\"MlpPolicy\", env, action_noise=action_noise, verbose=1, tensorboard_log=\"./hands-on_rl_tensorboard\")\n",
    "print(\"Starting training...\")\n",
    "td3_model.learn(total_timesteps=1000, callback=eval_callback, log_interval=1, tb_log_name=\"hands_on_rl_TD3\")\n",
    "td3_model.save('./saved_models/RL_agent_TD3')\n",
    "print(\"Completed training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d165cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "# Reset environment\n",
    "# Create new test env\n",
    "test_env = YourDoubleEnv()\n",
    "\n",
    "\n",
    "test_trained_agent(test_env, model, render_opt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be13d346",
   "metadata": {},
   "source": [
    "## 2.3 Proximal Policy Optimization (PPO)\n",
    "\n",
    "PPO is motivated by the following question: how can we take the biggest possible improvement step on a policy using the data we currently have, without stepping so far that we accidentally cause performance collapse? PPO is a family of first-order methods that use different tricks to keep new policies close to old. PPO methods are simpler to implement than other similar methods (such as TRPO), while empirically seeming to still perform well.\n",
    "\n",
    "In contrast with the previous methods, PPO learns in an on-policy manner. It can handle discrete or continuous action spaces.\n",
    "\n",
    "PPO is also specifically designed to allow speed-up of training by training several agents in parallell environments.\n",
    "\n",
    "Spinning Up: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
    "\n",
    "Stable Baselines 3 docs: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
    "\n",
    "Original paper: https://arxiv.org/pdf/1707.06347v2.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca16e48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "\n",
    "# Separate evaluation env\n",
    "eval_env = YourDoubleEnv()\n",
    "\n",
    "# Use deterministic actions for evaluation\n",
    "eval_callback = EvalCallback(eval_env,  best_model_save_path='./RL_logs/PPO',\n",
    "    log_path='./RL_logs/', eval_freq=1000,\n",
    "    deterministic=True, render=False)\n",
    "\n",
    "\n",
    "# Parallel environments. This is using a \"simple\" vectorized environment, simply calling \n",
    "# steps in four separate environments in sequence. No multithreading, but it is possible\n",
    "# to achieve in SB3.\n",
    "env = make_vec_env(YourDoubleEnv, n_envs=4)\n",
    "print(env)\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "ppo_model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./hands-on_rl_tensorboard\")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "ppo_model.learn(total_timesteps=25000, callback=eval_callback, log_interval=1, tb_log_name=\"hands_on_rl_PPO\")\n",
    "ppo_model.save('./saved_models/RL_agent_PPO')\n",
    "print(\"Completed training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c01d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "# Reset environment\n",
    "# Create new test env\n",
    "test_env = YourDoubleEnv()\n",
    "\n",
    "\n",
    "test_trained_agent(test_env, ppo_model, render_opt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdafa308",
   "metadata": {},
   "source": [
    "# Want to learn more?\n",
    "\n",
    "If you want to learn more about the theory of reinforcement learning there are many articles and tutorials available online.\n",
    "\n",
    "There is a very good textbook by Sutton and Barto called \"Reinforcement Learning:\n",
    "An Introduction\". You can access the second edition online and find more information on the following link: http://incompleteideas.net/book/the-book.html. Gives a very thorough introduction to RL and traditional methods.\n",
    "\n",
    "Other websites with lots of information are:\n",
    "\n",
    "- https://spinningup.openai.com/en/latest/ : an educational resource produced by OpenAI that makes it easier to learn about deep reinforcement learning (deep RL). Also provides a library with implementations of different DRL algorithms. Their documentations provide very nice and succinct explanations of the different tricks and methods used in different algorithms.\n",
    "- https://stable-baselines3.readthedocs.io/en/master/index.html: documentation of Stable Baselines 3. More on the technical side, but very useful when you actually want to start implementing your own environments.\n",
    "- https://deeplearningcourses.com/o/reinforcement-learning-control : A roadmap of courses to learn how to master RL and control problems. The courses are expensive, but you could use this roadmap as a guide to what would be beneficial to study.\n",
    "- https://neptune.ai/blog/best-reinforcement-learning-tutorials-examples-projects-and-courses: a blog post on neptune.ai with links to several different example projects and introductory courses.\n",
    "\n",
    "If you google whatever machine learning topic you are interested in, you are also likely to find many articles about it on sites such as medium or towardsdatascience. One does however need to keep in mind that the authors of these articles are often students and other beginners, so make sure to check who has written the article and take their conclusions with a grain of salt. Happy learning!\n",
    "\n",
    "I hope you enjoyed this small hands-on and if you have any further questions let me know.\n",
    "\n",
    "/ Joel Wulff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0217eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup env\n",
    "class YourDoubleEnv(gym.Env):\n",
    "    \n",
    "    # Define your action and observation spaces. You want to take actions in one dimension, changing the h42 phase offset,\n",
    "    # and want a 4D observation, consisting of your relative bunch lengths and bunch intensities. Both your observations and \n",
    "    # actions will be should be normalized to be within [-1,1] for optimisation reasons.\n",
    "    \n",
    "    metadata= {'render.modes': ['human']}\n",
    "    \n",
    "    action_space = gym.spaces.Box(\n",
    "                                low = np.array([-1,]),#__, # Fill in the __ with your action settings.\n",
    "                                high = np.array([1,]),#__,\n",
    "                                shape=(1,),\n",
    "                                dtype=np.float32)\n",
    "\n",
    "\n",
    "    ### Define what the observations are to be expected\n",
    "    observation_space = gym.spaces.Box(\n",
    "                                low = np.array([-1,-1,-1,-1]), # Fill in the __ with your observation settings.\n",
    "                                high = np.array([1,1,1,1]),\n",
    "                                shape=(4,),\n",
    "                                dtype=np.float32)\n",
    "    \n",
    "    ### Stop criteria (constituting \"good\" splittings with provided difference estimate).\n",
    "    BUNCH_LENGTH_INT_CRITERIA = 0.02 # Empirically evaluated diff_estimate that constitutes a \"good\" bunch splitting. Lower means longer training time, but smaller spread in bunch lengths/intensities.\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def __init__(self,\n",
    "                max_steps = 100,\n",
    "                max_step_size = 20,\n",
    "                min_setting = -45,\n",
    "                max_setting = 45,):\n",
    "        \n",
    "        ### Assign hyperparameter settings to attributes\n",
    "        self.max_steps = max_steps\n",
    "        self.max_step_size = max_step_size\n",
    "        self.min_setting = min_setting\n",
    "        self.max_setting = max_setting\n",
    "        \n",
    "        ### Status of the iterations\n",
    "        # Steps, initializing lists to store actions/states/rewards...\n",
    "        \n",
    "        self.step_counter = 0 # Counts the number of steps taken in the environment!\n",
    "        self.curr_step = -1  ## Not used ?\n",
    "        self.phase_correction = 0\n",
    "\n",
    "        # Initialize lists for tracking episodes\n",
    "        self.curr_episode = -1\n",
    "        self.action_episode_memory = []\n",
    "        self.diff_estimate_memory = []\n",
    "        self.state_memory = []\n",
    "        self.phase_set_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.is_finalized = False\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        One step/action in the environment, returning the observable\n",
    "        and reward. \n",
    "\n",
    "        Stopping conditions: max_steps reached, or splitting good enough.\n",
    "        \"\"\"\n",
    "        success = False    \n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code below!!\n",
    "        ################################################################################################################################################################\n",
    "        \n",
    "        # Hint: Number of steps taken in environment is tracked in self.step_counter\n",
    "        \n",
    "        \n",
    "        self._take_action(action) # Actually take action: Define your _take_action function below!! \n",
    "        reward = self._get_reward() # Get your reward: Define your _get_reward function below!! Returns a reward value.\n",
    "        \n",
    "        state = self.state\n",
    "        ### Check exit criteria: Achieved good enough state, or taken too many steps.\n",
    "        ### BUNCH_LENGHT_INT_CRITERIA based on the given diff_estimate value.\n",
    "        \n",
    "        if abs(self.diff_estimate) < self.BUNCH_LENGTH_INT_CRITERIA: # Check if the diff_estimate is below criterion. If so, episode finalized and a success!\n",
    "            self.is_finalized = True \n",
    "            success = True\n",
    "        # print(self.step_counter)    \n",
    "        if self.step_counter >= self.max_steps: # Check if you have exceeded the maximum step limit. If so, episode finalized but not a success...\n",
    "            self.is_finalized = True\n",
    "\n",
    "        # Here you can add any extra info you would like to be returned on each step, e.g. episode steps, rewards, actions etc.\n",
    "        info = {'success': success, 'steps': self.step_counter, 'profile': self.profile} \n",
    "        \n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code above!!\n",
    "        ################################################################################################################################################################\n",
    "        \n",
    "        \n",
    "        return state, reward.astype(np.float64), self.is_finalized, info # Standardized output according to gym framework.\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        \"\"\"\n",
    "        Actual action funtion.\n",
    "\n",
    "        Action from model is scaled to be between [-1,1] for better optimization performance. \n",
    "        Converted back to phase setting in degrees using self.max_step_size.\n",
    "        \n",
    "        Args:\n",
    "            action (ndarray): n-dimensional action. Datatype, dimension, and value ranges defined in self.action_space.\n",
    "        \"\"\"\n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code below!!\n",
    "        ################################################################################################################################################################\n",
    "\n",
    "        converted_action = action*self.max_step_size # Convert your action from your normalized range [-1,1] back into degrees using self.max_step_size. HINT: action*self.max_step_size = what?\n",
    "        self.phase_correction += converted_action # Phase correction tracks previous actions taken to get the cumulative change from start.\n",
    "\n",
    "        # Phase offset as action, add offset to current phase_set to get next setting. This is what defines which simulated datapoint to collect in your _get_state function!!!\n",
    "        self.phase_set += converted_action\n",
    "\n",
    "        # Update the self.state parameter with the new state. The preprovided self._get_state() will provide you with a state based on the current self.phase_set attribute value.\n",
    "        # The state will consist of a vector of 4 values: [bunch_length_1, bunch_length_2, bunch_intensity_1, bunch_intensity_2].\n",
    "        self.state = self._get_state()\n",
    "\n",
    "\n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code above!!\n",
    "        ################################################################################################################################################################  \n",
    "        curr_state = self.state.copy()\n",
    "        curr_phase_set = self.phase_set.copy()\n",
    "        self.action_episode_memory.append(action)\n",
    "        self.state_memory.append(curr_state)\n",
    "        self.phase_set_memory.append(curr_phase_set)\n",
    "        \n",
    "        self.step_counter += 1\n",
    "\n",
    "     \n",
    "    \n",
    "    def _get_state(self):\n",
    "        '''\n",
    "        Get the observable for a given phase_set. This function is provided completed to help you collect datapoints from the pre-simulated dataset.\n",
    "        The data_class class is written to provide datapoints from a quadsplit dataset, but by always providing h84=0 we only vary the first phase\n",
    "        offset.\n",
    "\n",
    "        Comment: The edge cases of trying to move to datapoints outside the simulated dataset needs to be handled.\n",
    "        Currently it is simply checked whether the phase setting is above the max setting or below the min setting,\n",
    "        and if so a pre-defined dummy observation is presented. The important factor to consider is to make sure that\n",
    "        all edge cases are covered by some dummy state, and that the dummy states are unique (so the model can learn\n",
    "        what steps to take to get back in the right search area). It is also highly advised to give an additional penalty\n",
    "        in the reward if the agent steps outside our region of simulated data.\n",
    "        '''\n",
    "        \n",
    "        ### Check whether we are within simulated settings\n",
    "        if (self.phase_set<self.min_setting):\n",
    "            state = np.array([0.5, -0.5, 0.5, -0.5])\n",
    "        elif (self.phase_set>self.max_setting):\n",
    "            state = np.array([-0.5, 0.5, -0.5, 0.5])\n",
    "        else:\n",
    "            \n",
    "            ################################################################################################################################################################\n",
    "            # Implement the code below!!\n",
    "            #########################################################################################################################\n",
    "            \n",
    "            # Collecting the simulated datapoint, calculating state description\n",
    "            profile = data_class.get_interpolated_profile(self.phase_set) # Second phase does not affect the first. Since we only care about h42, no need to assign h84 offfset.\n",
    "            state = get_state_from_profile(profile)\n",
    "            \n",
    "            ################################################################################################################################################################\n",
    "            # Implement the code above!!\n",
    "            ################################################################################################################################################################\n",
    "        \n",
    "            self.profile = profile # Add a tracking of the profile for plotting purposes!\n",
    "        return state.astype(np.float32)\n",
    "    \n",
    "    def _get_reward(self):\n",
    "        \"\"\" Evaluating the reward from the observable/state. \n",
    "            The example reward 'simple_profile' is provided, and is based on the final profile\n",
    "            after the splitting. This is using more information than just the state provided\n",
    "            to the agent.\n",
    "\n",
    "            Feel free to experiment and design your own reward as well! \n",
    "\n",
    "        Returns:\n",
    "            float: The reward based on the current state. \n",
    "        \"\"\" \n",
    "        \n",
    "        \n",
    "        \n",
    "        # The shape of your observable should match your optimization_space. For this excercise, it is expected\n",
    "        # that you use an observation space of a vector with four values\n",
    "        observable = self.state\n",
    "\n",
    "        diff_estimate = get_diff_estimate_from_state(observable)\n",
    "        self.diff_estimate = diff_estimate\n",
    "\n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code below!!\n",
    "        ################################################################################################################################################################\n",
    "        \"\"\" \n",
    "         Define your own reward here. The diff_estimate provided above is provided \n",
    "         as a simple way to define the difference between your bunches length/intensity \n",
    "         after the splitting. You want this to be as small as possible, so a better reward\n",
    "         should be given for a smaller diff_estimate. There is already a pre-defined\n",
    "         criterion for the diff_estimate to be considered a \"good\" splitting provided in\n",
    "         the BUNCH_LENGTH_INT_CRITERIA attribute. \n",
    "         \"\"\"\n",
    "\n",
    "        reward = -diff_estimate #______ # Define your own reward here!! \n",
    "\n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code above!!\n",
    "        ################################################################################################################################################################\n",
    "        \n",
    "        ### Tracking of diff_estimate. Lets you use my render() function to observe your agent.\n",
    "        curr_diff_estimate = self.diff_estimate.copy()\n",
    "        self.diff_estimate_memory.append(curr_diff_estimate)\n",
    "        self.reward_memory.append(reward)\n",
    "            \n",
    "        return reward\n",
    "       \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset to a random state to start over a training episode.\n",
    "        This function will be called everytime an episode is started\n",
    "        to provide an initial state.\n",
    "        \n",
    "        Returns:\n",
    "            state: The inital state of the environment. Should match the shape defined in observation_space.\n",
    "        \"\"\"\n",
    "        # Resetting to start a new episode\n",
    "        self.curr_episode += 1\n",
    "        self.step_counter = 0\n",
    "        self.is_finalized = False # This tracks wether an episode is complete or not.\n",
    "        \n",
    "        #print(f'Resetting for episode {self.curr_episode}')\n",
    "\n",
    "        # Initializing episode lists to track data for individual episodes. Some used for rendering.\n",
    "        self.action_episode_memory = []\n",
    "        self.state_memory= []\n",
    "        self.phase_set_memory= []\n",
    "        self.reward_memory= []\n",
    "        self.diff_estimate_memory= []\n",
    "        \n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code below!!\n",
    "        ################################################################################################################################################################\n",
    "        \n",
    "        # Getting initial state\n",
    "        \n",
    "        # Initialize random starting phase_set within [self.min_setting, self.max_setting], for example using random.uniform(min,max)\n",
    "        self.phase_set = random.uniform(self.min_setting,\n",
    "                                            self.max_setting)\n",
    "        \n",
    "        self.initial_offset = np.copy(self.phase_set)\n",
    "        self.phase_correction = 0\n",
    "                                        \n",
    "        self.state = self._get_state() # call _get_state to get the initial state from the starting phase_set.\n",
    "        state = self.state\n",
    "        \n",
    "        ################################################################################################################################################################\n",
    "        # Implement the code above!!\n",
    "        ################################################################################################################################################################\n",
    " \n",
    "        ### Some tracking of state, phase, reward, diff_estimate. Lets you use my render() function to observe your agent.\n",
    "        self.state_memory.append(state)\n",
    "        self.phase_set_memory.append(self.phase_set)\n",
    "        reward = self._get_reward()\n",
    "        self.reward_memory.append(reward)\n",
    "        \n",
    "        return state.astype(np.float32)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"\n",
    "        Set the random seed. Useful if you want to standardize trainings.\n",
    "        \"\"\"\n",
    "        \n",
    "        random.seed(seed)\n",
    "        np.random.seed\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Rendering function meant to provide a human-readable output. Base function in gym\n",
    "        environments to override. I provide a simple version that should let you observe \n",
    "        your trained agent during evaluation.\n",
    "        \"\"\"\n",
    "        plt.figure('Agent')\n",
    "        plt.clf()\n",
    "        plt.subplot(131)\n",
    "        plt.suptitle(f'Episode {self.curr_episode}')\n",
    "        plt.title('Current profile')\n",
    "        plt.plot(self.profile,'b')\n",
    "        plt.subplot(132)\n",
    "        plt.title('Difference estimate')\n",
    "        plt.plot(self.diff_estimate_memory, 'o-')\n",
    "        plt.axhline(y=self.BUNCH_LENGTH_INT_CRITERIA, color='k', linestyle='--')\n",
    "        plt.subplot(133)\n",
    "        plt.title('h42 phase offset')\n",
    "        plt.plot(np.asarray(self.phase_set_memory, dtype=object), 'go-')\n",
    "        plt.axhline(y=0, color='k', linestyle='--')\n",
    "        plt.ylim((-30,30))\n",
    "        \n",
    "        #plot_finish(fig=fig, axes=axes, xlabel='Setting', ylabel='Observable')\n",
    "        plt.pause(0.2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cb8ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to stop all tensorboard processes on windows and clean temp dir.\n",
    "\n",
    "! powershell \"echo 'checking for existing tensorboard processes'\"\n",
    "! powershell \"ps | Where-Object {$_.ProcessName -eq 'tensorboard'}\"\n",
    "\n",
    "! powershell \"ps | Where-Object {$_.ProcessName -eq 'tensorboard'}| %{kill $_}\"\n",
    "\n",
    "! powershell \"echo 'cleaning tensorboard temp dir'\"\n",
    "! powershell \"rm $env:TEMP\\.tensorboard-info\\*\"\n",
    "\n",
    "! powershell \"ps | Where-Object {$_.ProcessName -eq 'tensorboard'}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa75d8b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c3a8bfbd7fd94e6c5875dc94d11f6d20ee55113703ee5cc439085bac8c32f553"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
